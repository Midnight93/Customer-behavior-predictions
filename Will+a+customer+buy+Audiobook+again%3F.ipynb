{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem:   \n",
    "* Each customer in the database has made a purchase at least once\n",
    "* Create a machine learning algorithm based on given data that can predict if a customer will buy again from the Audiobook company\n",
    "* We want to focus on customers who are likely to come back and buy again\n",
    "* This model can identify the most important metrics for a customer to come back again\n",
    "* The inputs:    \n",
    "\n",
    "1)\tID: customer Id; no information in here. We will skip this in the algorithm  \n",
    "2)\tBook length (mins)_overall: sum of the lengths of all purchased books  \n",
    "3)\tBook length(mins)_avg: sum of the lengths of all purchased books / number of purchased books. So if somebody bought a single book, the average length and the overall length will be the same.  \n",
    "4)\tPrice overall: sum of the prices of all purchased books  \n",
    "5)\tPrice_avg: sum of the prices of all purchased books / number of purchased books  \n",
    "6)\tReview: it's a Boolean. It shows if a customer left a review. Our assumption is people who leave reviews are more likely to come back again  \n",
    "7)\tReview 10/10: measure of the review on a scale of a 1 to 10. \n",
    "\n",
    "      a. First preprocessing trick: logically we will only have a value for people who left a review. Most people leave no review. We substitute all the missing values with the average review. The average is 8.91. For our ML algorithm, 8.91 would mean the status quo. A review more than that indicate above average feelings. \n",
    "  \n",
    "8)\tMinutes listened: this is a measure of engagement  \n",
    "9) Completion: total minutes listened / book length overall  \n",
    "10) Support requests: the total number of support requests a person has opened. It may mean more support the person needed the more he or she got fed up with the platform and abandoned it.   \n",
    "11) Last visited minus purchase date: the bigger the difference the better. If the value is 0, the customer has never accessed what he has bought  \n",
    "12)\tTargets: 1 if a person converted and 0 if it's not. (=purchased again or not). After the two year period, targets are measured with the additional 6 months  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocess the data\n",
    "* a) balance the dataset\n",
    "* b) create 3 datasets: training, validation and test\n",
    "* c) save the newly created sets in tensor friendly format = npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should do the trick for most datasets organized in the way: many inputs, and then 1 cell containing the targets (supersized learning datasets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter =',')\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "#All rows, and column 1 to last column (excluded). we exclude the first column (0), because we will not use ID column\n",
    "#We exclude the last column (-1) because we will assign targets separately.\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we have unbalanced dataset, since most of the customers didn't convert in the given time\n",
    "#we need to balance it with 50% convert and 50% not convert\n",
    "#we need to count how many targets are 1 (meaning customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all)) #2237 targets are 1 out of 14,084 (targets_all.shape[0] gives the number of rows)\n",
    "\n",
    "#set a counter for targets that are 0\n",
    "zero_targets_counter = 0\n",
    "#we want to create a balanced dataset, so we will have to remove some indices:\n",
    "indices_to_remove=[]\n",
    "\n",
    "#count the number of targets that are 0.\n",
    "#once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]): #targets_all.shape[0]: returns the number of rows in the dataset\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1 #ztc=ztc+1 \n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "        \n",
    "#create two new variables, one that will contain the inputs, one that will contain the targets\n",
    "#we delete all the indices that we marked \"to remove\" in the loop above\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "#np.delete (input array, slice, axis (0=rows))\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the inputs: scaling\n",
    "Different order of magnitudes creates problems, so we will standardize the inputs: We will use sklearn for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the data:\n",
    "When the data was collected it was actually arranged by date. We will shuffle the indices of the data, so the data is not arranged in any way when we feed it.  \n",
    "\n",
    "Since we will be batching, we want the data to be as rondomly spread out as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0]) #gives indices numbers from 0 to 4473 = 4474 numbers which is the range. (scaled_inputs.shape[0] = 4474) \n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "#use the shuffled indices to shuffle the inputs and targets\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Split the dataset: train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1815.0 3579 0.5071248952221291\n",
      "225.0 447 0.5033557046979866\n",
      "197.0 448 0.43973214285714285\n"
     ]
    }
   ],
   "source": [
    "#count the total number of samples:\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "#count the samples in each subset, we want 80-10-10 distribution: train, validation, test\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "#the test dataset contains all remaining data\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "#create variables that record the inputs and targets for training\n",
    "#in our shuffled dataset, they are the first 'train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "#create variables that record the inputs and targets for the validation\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count + validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count + validation_samples_count]\n",
    "\n",
    "#create variables that record the inputs and targets for the test\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]\n",
    "\n",
    "#print the number of targets that are 1s, the total number of samples, and the proportion for training, validation and test:\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Save the three datasets in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets= test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Create a class that handles batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you want to batch the data you need to have appropriate methods. There are some batching methods integrated in TensorFlow (tf.train.batch()) and sklearn, but some problems may need specific coding. You can use these following methods for any machine learning framework you need (directly or after little fine-tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a class that will do the batching for the algorithm\n",
    "#this code is extremely reusable. You should just change the Audiobooks_data everywhere in the code\n",
    "class Audiobooks_Data_Reader():\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        npz = np.load(\"Audiobooks_data_{0}.npz\".format(dataset))\n",
    "        #if I call this class with x('train', 5), it will load 'Audiobooks_data_train.npz' with a batch size of 5\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        #two variables that take the values of the inputs and targets.\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape [0] // self.batch_size\n",
    "        #this counts the batch number, given the batch size\n",
    "        #if the batch size is None (we call the class without entering batch size), we are either validating or testing, so we want to take the data in a single batch \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_batch>=self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "    #this is a method which loads the next batch\n",
    "    #now we slice the dataset in batches and then the next function loads them one after the other\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        target_batch = self.targets[batch_slice]\n",
    "        self.curr_batch +=1\n",
    "    #one-hot encoding for the targets. this is useful for classification task with more than one target\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((target_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(target_batch.shape[0]), target_batch] = 1\n",
    "    #the function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "    #a method needed for iterating over the batches, as we will put them in a loop\n",
    "    #this following code tells python that the class we're defining is iterable \n",
    "    #an iterator in python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching explanation:\n",
    "We created the class Audiobooks_Data_Reader. There are three functions in the class: init, next and iter.  \n",
    "This class will be an iterator. An iterator is a class with methods: next and iter; and it is used with loops.  \n",
    "The init method loads the data from npz.  \n",
    "The next method loads the next batch from the npz.  \n",
    "The iter tells python that the class is iterable.  \n",
    "\n",
    "##### INIT METHOD: \n",
    "this method has two arguments: dataset and batch size. The self is simply python notation that defines the method as an instance method as opposed to a static method. instance method: you can apply the method to instances in the class. statis method: you can apply the method to the class. if we don't declare a batch size, the class will assume it has to load all the data as a single batch.\n",
    "###### example: \n",
    " x = Audiobooks_Data_Reader('train', 5)  \n",
    "the result would be to load the train data set and then proceed with the operations taking batches of five samples at a time\n",
    "##### batch size = how many samples in each batch\n",
    "##### batch number = how many batches in total\n",
    "y = Audiobooks_Data_reader('validation')  \n",
    "the result would be to load the validation dataset where the whole data is contained in a single batch  \n",
    "\n",
    "##### NEXT FUNCTION:\n",
    "Next function slices the next batch out of the dataset and loads it for the next iteration. In addition, this is the place where we one-hot encode the targets. as the targets were 0s and 1s we want to split them into one-hot encoded target. we used the parameter called classes number. there are two classes: 0s and 1s. if you have a classification problem with more than two classes (bread, yoghurt, muffin) then you should change the class_num variable\n",
    "\n",
    "##### The differences we should make for other datasets:\n",
    "1) adjust the names of the NPZ files  \n",
    "2) adjust the number of classes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Create the machine learning algorithm\n",
    "We will create an algorithm which is essentially copy-pasting the MNIST code and we will simply adjust where needed. tensorflow code is extremely reusable. we will put the wholde code in one piece as we can simply rerun the cell and train a new model. That's because whole algorithm is contained in the cell and we have the tf.reset_default_graph() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.533. Validation loss: 0.440. Validation accuracy: 76.96%\n",
      "Epoch 2. Training loss: 0.394. Validation loss: 0.401. Validation accuracy: 77.63%\n",
      "Epoch 3. Training loss: 0.356. Validation loss: 0.378. Validation accuracy: 78.97%\n",
      "Epoch 4. Training loss: 0.341. Validation loss: 0.369. Validation accuracy: 79.87%\n",
      "Epoch 5. Training loss: 0.332. Validation loss: 0.367. Validation accuracy: 79.64%\n",
      "Epoch 6. Training loss: 0.326. Validation loss: 0.364. Validation accuracy: 80.09%\n",
      "Epoch 7. Training loss: 0.320. Validation loss: 0.362. Validation accuracy: 80.09%\n",
      "Epoch 8. Training loss: 0.316. Validation loss: 0.360. Validation accuracy: 79.42%\n",
      "Epoch 9. Training loss: 0.314. Validation loss: 0.361. Validation accuracy: 79.42%\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#the hyperparameters\n",
    "input_size = 10 #number of columns we use\n",
    "output_size = 2 # as we one-hot encoded the targets\n",
    "hidden_layer_size = 200\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#the placeholders:\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "#outline the model with 2 hidden layers:\n",
    "weights_1 = tf.get_variable('weights_1', [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable('biases_1', [hidden_layer_size])\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "#no activation output\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "#softmax cross entropy loss with logits:\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "#get a 0 or 1 for every input indicating whether it output the correct answer\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "#optimize with Adam:\n",
    "optimize = tf.train.AdamOptimizer(learning_rate = 0.003).minimize(mean_loss)\n",
    "\n",
    "#create a session:\n",
    "sess= tf.InteractiveSession()\n",
    "\n",
    "#initialize the variables:\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "#other hyperparameters\n",
    "batch_size=500\n",
    "max_epochs =50\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "#load the first batch of training and validation, using the class we created\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "#optimize the algorithm: create for loop for epochs:\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss=0.\n",
    "    for input_batch, target_batch in train_data: #iterate over the training data\n",
    "        _, batch_loss = sess.run([optimize,mean_loss],\n",
    "                feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "        curr_epoch_loss +=batch_loss #record the batch loss into the current loss\n",
    "    curr_epoch_loss /=train_data.batch_count #find the mean curr_epoch_loss\n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    for input_batch, target_batch in validation_data: #use the same logic of the code to forward propagate the validation set \n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "            feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    prev_validation_loss = validation_loss\n",
    "        \n",
    "print('End of training')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Test the Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:82.59%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data: # we need the forwardpropagate as we did in the validation. cpy and past the validation forward propagate change the names and change the second line \n",
    "        test_accuracy = sess.run([accuracy],\n",
    "            feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "test_accuracy_percent = test_accuracy[0] *100.\n",
    "#test_accuracy[0]: because it is a list with one value only\n",
    "print('test accuracy:' + '{0:.2f}'.format(test_accuracy_percent) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is not sufficient, so we don't expect more than 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer_size= 50; test accur = 81.70  \n",
    "hidden_layer_size = 80; test accur = 84.38  \n",
    "hidden_layer_size = 95; test accur = 84.60  \n",
    "hidden_layer_size = 80; sigmoid both output; test_accur= 81.47 (more epochs)  \n",
    "hidden_layer_size = 80; learning rate = 0.002; test_accur = 84.15  \n",
    "hidden_layer_size = 80; learning rate = 0.002; batch size=1000; test acc= 81.70  \n",
    "hidden_layer_size = 100; learning rate = 0.002; batch size=100; test acc = 84.38  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
